# tasklist.md - GlovePost Development Tasks (Updated March 09, 2025)

## Content Aggregation
- **Completed:**
  - [x] Implement RSS feed parser in Python
  - [x] Set up content categorization based on keywords
  - [x] Store content in MongoDB with proper schema
  - [x] Add content refresh script to update database with fresh content
  - [x] Implement content sorting by timestamp
  - [x] Implement web scraping for X/Twitter content (no paid API)
    - [x] Use `beautifulsoup4` to scrape public X pages (e.g., news accounts)
    - [x] Parse tweets for `title`, `content_summary`, `url`, `timestamp`
    - [x] Add fallback to Nitter instances for more reliable scraping
  - [x] Implement web scraping for Facebook content
    - [x] Scrape public Facebook pages (e.g., news outlets) using `requests` and `beautifulsoup4`
    - [x] Handle basic content extraction (posts, links)
    - [x] Add fallback to mbasic.facebook.com mobile version
  - [x] Add additional media sources through combined RSS and web scraping
  - [x] Improve content filtering:
    - [x] Add duplicate detection (e.g., URL, title matching, content similarity)
    - [x] Enhance quality filter (e.g., minimum word count, spam keyword blacklist)
    - [x] Implement advanced filtering algorithms to strip away noise and fluff
    - [x] Develop heuristic rules to identify and remove low-quality content (e.g., clickbait, ads)
- **Pending:**
  - [ ] Integrate more advanced natural language processing (NLP) techniques to assess content relevance and quality
  - [ ] Implement tracking of source reliability and content quality over time
  - [ ] Add support for image content analysis and extraction
  - [ ] Create a scheduler for periodic content updates
- **New Tasks (4chan and Reddit Integration):**
  - [x] Implement web scraping for 4chan content
    - [x] Use `requests` and `beautifulsoup4` to scrape threads from public 4chan boards (e.g., /g/, /news/)
    - [x] Parse posts for `title` (thread subject), `content_summary` (post text), `url` (thread link), `timestamp`
    - [x] Handle anonymous posting by assigning a generic source (e.g., "4chan")
  - [x] Implement web scraping for Reddit content
    - [x] Use `requests` and `beautifulsoup4` to scrape public subreddits (e.g., r/news, r/technology)
    - [x] Parse posts for `title`, `content_summary` (self-text or link description), `url`, `timestamp`
    - [x] Avoid Reddit API to keep costs down, relying on scraping within legal bounds
- **New Tasks (from Research - Search Functionality):**
  - [ ] Set up indexing for efficient searching
    - [ ] Use MongoDB text search or integrate ElasticSearch for advanced search capabilities
    - [ ] Index content fields like `title`, `content_summary`, and `category` for full-text search
- **New Tasks (Script Improvements - `content_aggregator.py`):**
  - [ ] Implement real X/Twitter scraping in `fetch_x_posts`
    - [ ] Replace mock data with `beautifulsoup4` scraping using Nitter instances
    - [ ] Add retry logic with fallback to mock data if scraping fails
  - [ ] Implement real Facebook scraping in `fetch_facebook_posts`
    - [ ] Replace mock data with `beautifulsoup4` scraping via `mbasic.facebook.com`
    - [ ] Handle rate limits with proxy rotation or mock fallback
  - [ ] Enhance `categorize_content` with NLP
    - [ ] Replace keyword matching with NLP models (e.g., spaCy, NLTK) for context-aware categorization
    - [ ] Train on sample data from all sources
  - [x] Parallelize content fetching in `store_content`
    - [x] Use `concurrent.futures` or `asyncio` for simultaneous source fetching
    - [x] Implement rate limiting to avoid server overload
  - [ ] Validate and normalize content in `clean_content`
    - [ ] Add URL validation (e.g., `urlparse`), timestamp consistency, and required field checks
    - [ ] Normalize text (e.g., remove extra whitespace, standardize encoding)
  - [ ] Handle external scraper failures robustly
    - [ ] Validate JSON output from `reddit_scraper.py` and `4chan_scraper.py`
    - [ ] Add timeout and retry logic for subprocess calls
  - [ ] Improve mock data realism
    - [ ] Generate dynamic mock data with source-specific patterns (e.g., Reddit upvotes, 4chan style)
- **New Tasks (Script Improvements - `4chan_scraper.py`):**
  - [x] Include reply content in `parse_thread_page`
    - [x] Aggregate replies to enrich `content_summary` (e.g., top 3 replies by length)
    - [x] Update cleaning logic to handle reply-specific formatting (e.g., quotes)
  - [x] Enhance timestamp parsing in `extract_timestamp`
    - [x] Support multiple 4chan timestamp formats (e.g., with/without day)
    - [x] Log unparseable timestamps for manual review
  - [ ] Implement global rate limiting in `fetch_4chan_content`
    - [ ] Add a rate limiter (e.g., `ratelimit` library) to cap requests per minute
    - [ ] Support proxy rotation for large-scale scraping
  - [ ] Use dynamic categorization in `get_category_from_board`
    - [ ] Analyze thread content with NLP instead of static `CATEGORIES_MAPPING`
    - [ ] Combine board and content analysis for hybrid categorization
  - [x] Parallelize thread detail fetching in `fetch_threads_from_board`
    - [x] Use `concurrent.futures` to fetch thread details concurrently
    - [x] Maintain randomized delays to avoid detection
  - [ ] Validate content quality in `fetch_4chan_content`
    - [ ] Ensure `content_summary` meets minimum length post-cleaning (e.g., 50 chars)
    - [ ] Flag low-quality items for review
- **New Tasks (Script Improvements - `reddit_scraper.py`):**
  - [x] Improve timestamp accuracy in `parse_reddit_posts`
    - [x] Correct `time_attr` parsing to use ISO format instead of Unix assumption
    - [x] Enhance `estimate_unix_timestamp` with better unit precision (e.g., fetch exact post date)
  - [x] Enrich content with comments and link descriptions in `fetch_post_detail`
    - [x] Fetch top 3 comments for discussion posts
    - [x] Scrape link descriptions for non-self posts using URL content
  - [ ] Implement global rate limiting in `fetch_reddit_content`
    - [ ] Use `ratelimit` library to cap requests per minute
    - [ ] Add proxy rotation support for high-volume scraping
  - [ ] Use dynamic categorization in `get_category_from_subreddit`
    - [ ] Analyze post content with NLP instead of static `CATEGORIES_MAPPING`
    - [ ] Combine subreddit and content for hybrid categorization
  - [x] Parallelize post detail fetching in `fetch_posts_from_subreddit`
    - [x] Use `concurrent.futures` for concurrent detail fetching
    - [x] Maintain randomized delays to avoid detection
  - [ ] Enhance score parsing in `parse_reddit_posts`
    - [ ] Handle edge cases (e.g., "Vote", thousands separators) for accurate upvotes
    - [ ] Estimate downvotes based on upvote trends if possible
  - [ ] Make HTML parsing more resilient in `parse_reddit_posts`
    - [ ] Use flexible selectors or fallbacks for Reddit layout changes
    - [ ] Test against `old.reddit.com` updates

## Recommendation Engine
- **Completed:**
  - [x] Create Python-based recommendation engine
  - [x] Implement category-based content recommendations
  - [x] Connect recommendation engine to MongoDB for content retrieval
  - [x] Add basic scoring algorithm for content relevance
  - [x] Set up integration between Node.js backend and Python engine
  - [x] Create virtual environment for Python dependencies
  - [x] Add reasoning for recommendations ("Recommended because...")
  - [x] Enhance recommendation algorithm with more sophisticated scoring
    - [x] Add time decay factor (recent content scores higher)
  - [x] Add user interaction history to recommendation calculations
    - [x] Fetch interactions from PostgreSQL
  - [x] Implement keyword-based recommendations
    - [x] Extract keywords from content summaries
    - [x] Match with user-defined keywords in preferences
  - [x] Integrate thumbs up/thumbs down feedback into recommendation engine
    - [x] Update `recommendation_engine.py` to factor in user ratings
    - [x] Adjust content scores based on aggregated user feedback across all users
  - [x] Develop a user interface for tweaking recommendation parameters
    - [x] Add slider for users to adjust rating influence on recommendations
    - [x] Provide information explaining how the rating weight affects recommendations
  - [x] Implement a feedback loop where user interactions directly influence future recommendations
    - [x] Adjust recommendation scores based on user feedback (likes/dislikes)
- **Pending:**
  - [ ] Incorporate source reputation (e.g., weight trusted sources higher)
  - [ ] Add content diversity features to avoid recommendation bubbles
    - [ ] Ensure variety by limiting same-category recommendations (e.g., max 3 per category)
- **New Tasks (from Research - Twitter Algorithm):**
  - [ ] Implement machine learning model for recommendations
    - [ ] Use LightGBM or similar (e.g., XGBoost) for engagement prediction
    - [ ] Install dependencies (`lightgbm`, `scikit-learn`)
  - [ ] Define relevant features for the model
    - [ ] Extract features: content category, source, age, user preferences, interaction history
    - [ ] Source from MongoDB (content) and PostgreSQL (interactions)
  - [ ] Collect and preprocess data for training
    - [ ] Build dataset from interactions, label engagement (e.g., viewed, rated positively)
    - [ ] Split into training (80%) and validation (20%), handle missing data
  - [ ] Train and validate the model
    - [ ] Train using training set, evaluate with precision/recall/F1 on validation set
    - [ ] Tune hyperparameters if needed
  - [ ] Integrate model into recommendation engine
    - [ ] Update `recommendation_engine.py` to use ML model for scoring
    - [ ] Ensure Node.js compatibility via subprocess
  - [ ] Implement two-stage recommendation process
    - [ ] Add candidate generation: filter by category/source preferences
    - [ ] Rank candidates with ML model
  - [ ] Provide transparency through feature importance
    - [ ] Use LightGBM feature importance for explanations (e.g., "60% category match")
    - [ ] Integrate with UI visualization
- **New Tasks (from Research - Commenting and Search):**
  - [x] Integrate comment analysis into recommendation algorithm
    - [ ] Extract keywords or sentiment from comments using NLP (e.g., NLTK or spaCy)
    - [x] Adjust recommendation scores based on comment activity (e.g., highly discussed articles score higher)
  - [ ] Use search queries to influence recommendations
    - [ ] Log search queries and suggest related articles based on search history
    - [ ] Adjust recommendation scores for frequently searched articles with a decay factor
- **New Tasks (Script Improvements - `content_aggregator.py`):**
  - [x] Incorporate social metrics into recommendations
    - [x] Use Reddit upvotes/downvotes from `fetch_reddit_posts` to influence scores
    - [x] Store metrics in `clean_content` for recommendation engine use

## User Management
- **Completed:**
  - [x] Implement user preferences storage
  - [x] Create PostgreSQL integration for user data
  - [x] Add user consent tracking for privacy controls
  - [x] Implement API for updating user preferences
  - [x] Add user authentication (OAuth 2.0)
    - [x] Install `passport` and `passport-google-oauth20`
    - [x] Configure Google OAuth in `server.js`
    - [x] Secure endpoints with middleware
    - [x] Implement user registration and login functionality
    - [x] Add `POST /auth/register` and `POST /auth/login` endpoints
    - [x] Store hashed passwords in PostgreSQL (using `bcrypt`)
  - [x] Create user profile pages
    - [x] Add API endpoint `GET /user/profile/:id`
- **New Tasks (from Research):**
  - [ ] Allow users to influence and tweak the recommendation algorithm
    - [ ] Expose algorithm parameters in the user settings (e.g., weight sliders for different factors)
    - [ ] Provide clear documentation or tooltips explaining how each parameter affects recommendations
- **New Tasks (from Research - Commenting System):**
  - [ ] Implement comment moderation tools
    - [ ] Allow admins to delete/flag comments via `GET /admin/comments`
    - [ ] Implement reporting system (`POST /comments/report`) with admin notifications

## User Interactions
- **Completed:**
  - [x] Create interaction tracking system (views, clicks, etc.)
  - [x] Implement interaction recording in PostgreSQL
  - [x] Add API endpoints for retrieving user interaction history
  - [x] Support clearing interaction history for privacy
  - [x] Add thumbs up/thumbs down tracking to user interactions
    - [x] Extend `interactions` table in PostgreSQL with `rating` column
    - [x] Update `POST /interactions` endpoint to accept rating data
- **Pending:**
  - None (fully completed based on status)
- **New Tasks (from Research - Commenting System):**
  - [ ] Implement commenting system
    - [ ] Create PostgreSQL table for comments (`comment_id`, `user_id`, `article_id`, `comment_text`, `timestamp`)
    - [ ] Implement API endpoints: `POST /comments`, `GET /comments/article/:id`
- **New Tasks (Script Improvements - `content_aggregator.py`):**
  - [ ] Track fetch metadata
    - [ ] Add `fetched_at` field to items in `clean_content` for freshness analytics
    - [ ] Store fetch timestamps in PostgreSQL for aggregation stats
- **New Tasks (from Research - Twitter Algorithm):**
  - [ ] Enhance data collection for model training
    - [ ] Log additional interaction data (e.g., time spent on article)
    - [ ] Ensure GDPR compliance with user consent

## Frontend
- **Completed:**
  - [x] Create basic React application structure
  - [x] Implement Home page with content display
  - [x] Add Settings page for user preferences
  - [x] Create ContentCard component for displaying articles
  - [x] Connect frontend to backend API endpoints
  - [x] Add basic styling and layout
  - [x] Improve mobile responsiveness
    - [x] Add media queries in `index.css`, `App.css`, `Home.css`, and `Settings.css`
    - [x] Test on multiple device sizes
  - [x] Add content filtering by category/source in UI
    - [x] Create filter dropdowns in `Home.js`
    - [x] Update API calls with query params
  - [x] Add user feedback mechanism for recommendations
    - [x] Add "Like/Dislike" (thumbs up/down) buttons to `ContentCard`
    - [x] Send feedback to interaction endpoint
  - [x] Add dark mode theme
    - [x] Implement theme toggle in `App.js`
    - [x] Define dark mode styles in `index.css`
  - [x] Implement more sophisticated UI with glove/post imagery
    - [x] Add CSS-based icons for gloves and posts
    - [x] Style `ContentCard` to resemble gloves on posts
    - [x] Simplify card design by removing redundant elements
    - [x] Make entire card clickable to improve usability
  - **Pending:**
  - [ ] Create detailed article view page
    - [ ] Add `Article.js` component
    - [ ] Route to `/article/:url` with full content display
- **New Tasks (from Research):**
  - [x] Design the UI with the glove-on-post metaphor
    - [x] Use glove icons to represent articles and posts to represent categories
    - [x] Create visual elements where users can "pick up" gloves (articles) by clicking on them
  - [ ] Implement a "Lost and Found" section for user-saved articles
    - [ ] Allow users to save articles for later, inspired by finding lost gloves
- **New Tasks (Thumbs Up/Down Mechanism):**
  - [x] Add thumbs up/thumbs down buttons to `ContentCard` component
    - [x] Include icons for thumbs-up/down
    - [x] Send rating data to `POST /interactions` on click
  - [x] Display aggregated thumbs up/down counts on each post
    - [x] Fetch ratings from backend and show totals in UI
- **New Tasks (from Research - Commenting, Search, Accessibility):**
  - [ ] Add comment section to `Article.js` component
    - [ ] Display comments sorted by timestamp with user info
    - [ ] Add form for posting comments with validation
  - [ ] Implement search bar in `App.js` or `Home.js`
    - [ ] Add search input with autocomplete using `react-autosuggest`
    - [ ] Create `SearchResults.js` component for displaying results with pagination
  - [ ] Ensure all images have alt text
    - [ ] Add descriptive alt text to glove/post imagery and other images
    - [ ] Verify compliance with tools like WAVE or axe
  - [ ] Check and adjust color contrast for readability
    - [ ] Ensure WCAG contrast ratios (e.g., 4.5:1) in light and dark modes
    - [ ] Use `color-contrast` library for checks
  - [ ] Make site navigable using keyboard only
    - [ ] Ensure all interactive elements are focusable with `tabindex` and ARIA roles
    - [ ] Test with screen readers (e.g., NVDA, JAWS)
  - [ ] Provide clear labels and instructions for forms
    - [ ] Add `aria-label` and `aria-describedby` to forms (search, comments)
    - [ ] Include inline help text for complex inputs
- **New Tasks (from Research - Twitter Algorithm):**
  - [ ] Create a visualization of the recommendation process
    - [ ] Display breakdown (e.g., "60% category match") in `ContentCard`
    - [ ] Add interactive chart in `Settings.js` showing feature influence

## Infrastructure
- **Completed:**
  - None (no infrastructure tasks marked complete in status)
- **Pending:**
  - [ ] Set up automated testing
    - [ ] Install Jest in `backend` and `frontend`
    - [ ] Write unit tests for `content.js`, `user.js`, `recommendations.js`
    - [ ] Test frontend components (`Home`, `Settings`)
  - [ ] Configure production deployment
    - [ ] Build frontend (`npm run build`)
    - [ ] Serve via `backend/public` in `server.js`
    - [ ] Deploy to AWS with PM2
  - [ ] Implement caching for faster load times
    - [ ] Use Redis for content caching (`/content/latest`)
    - [ ] Cache recommendations for 1 hour
  - [ ] Set up monitoring and logging
    - [ ] Install Winston or Morgan for logging
    - [ ] Set up basic monitoring (e.g., uptime checks)
  - [ ] Optimize performance for high traffic
    - [ ] Conduct load testing with 1,000 concurrent users
  - [ ] Implement rate limiting for API endpoints
    - [ ] Use `express-rate-limit` to prevent abuse
- **New Tasks (from Research - Analytics):**
  - [ ] Integrate Google Analytics or similar tool
    - [ ] Set up Google Analytics for page views, sessions, and bounce rates
    - [ ] Configure events for key actions (e.g., article views, searches, comments)
  - [ ] Set up custom events for key user actions
    - [ ] Track "Article Viewed," "Comment Posted," "Search Performed" events
    - [ ] Store aggregated analytics in a separate database for reporting
- **New Tasks (Script Improvements - `content_aggregator.py`):**
  - [ ] Add configuration file support
    - [ ] Support JSON/YAML config file to override args (e.g., sources, limits)
    - [ ] Load defaults from `.env` or config if args unspecified
  - [ ] Implement robust error recovery
    - [ ] Add retry with exponential backoff for failed fetches/database writes
    - [ ] Store failed items in a queue for later retry
  - [ ] Optimize logging
    - [ ] Add log rotation with `RotatingFileHandler` to manage log size
    - [ ] Include content-specific logs (e.g., item titles) for debugging
- **New Tasks (Script Improvements - `4chan_scraper.py`):**
  - [ ] Make log file path configurable
    - [ ] Use an env variable or arg (e.g., `--log-dir`) for log directory
    - [ ] Ensure compatibility with `content_aggregator.py` subprocess calls
- **New Tasks (from Research - Twitter Algorithm):**
  - [ ] Ensure scalability for model training
    - [ ] Set up batch training (e.g., daily) on AWS SageMaker
    - [ ] Cache model outputs for quick retrieval
- **Completed Tasks (Continuous Scraping Improvements):**
- [x] Replace `refresh_content.sh` with multithreaded `refresh_content.py`
- [x] Implement task queue using `queue.Queue` for scraper jobs
- [x] Use `ThreadPoolExecutor` for parallel execution
- [x] Add continuous operation logic
- [x] Run as daemon with configurable interval (e.g., 15 min)
- [x] Support command-line args for workers and interval
- [x] Enhance robustness in `refresh_content.py`
- [x] Implement retries with exponential backoff for scraper failures
- [x] Centralize logging to `refresh_content.log`
- [x] Optimize resource usage
- [x] Add rate limiting between tasks (e.g., `time.sleep`)
- [x] Test with varying worker counts (e.g., 2, 4, 8)

## Next Steps Priority
1. ✅ **Implement User Authentication (OAuth 2.0)** (COMPLETED)
   - Essential for personalization and security, supports proper user identification
   - High priority as it affects multiple features (comments, profiles) and user experience
   - Enabled proper user profile management and secure data access
2. ✅ **Enhance Recommendation Engine to Use Rating Data** (COMPLETED)
   - Integrated thumbs up/down feedback into the recommendation algorithm
   - Created feedback loops where user ratings influence content recommendations
   - Implemented weighting of content based on community ratings
3. ✅ **Add Dark Mode Theme** (COMPLETED)
   - Improved accessibility and user experience, especially for nighttime reading
   - Created smooth transition between light and dark modes
   - Implemented theme persistence using localStorage
4. ✅ **Implement the Glove-on-Post Visual Metaphor** (COMPLETED)
   - Created a distinct visual identity for the application
   - Designed and implemented CSS-based icons for the metaphor
   - Styled ContentCard components with glove imagery and improved UX
5. **Implement Machine Learning Model for Recommendations**
   - Transition to LightGBM-based system inspired by Twitter’s algorithm
   - Define features, train model, and integrate for personalized recommendations
6. ✅ **Replace refresh_content.sh with Multithreaded refresh_content.py** (COMPLETED)
   - Created Python-based daemon with parallel processing
   - Implemented thread pools for improved efficiency
   - Added systemd service for automated startup and monitoring
   - Enhanced individual scrapers with parallel processing

## Additional Notes
- **Research Integration:** Enhanced recommendation engine with Twitter’s ML approach (LightGBM, two-stage process), adding scalability and transparency tasks.
- **Scraping Compliance:** Web scraping for X, Facebook, 4chan, and Reddit must comply with terms of service and robots.txt. Fallbacks (e.g., Nitter) improve reliability.
- **User Empowerment:** ML model transparency, thumbs up/down, commenting, and search empower users to control their experience.
- **Thematic Design:** Glove-on-post metaphor ties into UI and recommendation explanations (e.g., "raising" content via ML scores).
- **Accessibility and Analytics:** Tasks ensure inclusivity and data-driven improvements, aligning with best practices.